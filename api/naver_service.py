import os
import requests
import json
from dotenv import load_dotenv, find_dotenv
from bs4 import BeautifulSoup
import time
import hmac
import hashlib
import base64
import re
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET

load_dotenv()

def get_realtime_keywords():
    """
    Nate ì‹¤ì‹œê°„ ì´ìŠˆ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        url = "https://www.nate.com/js/data/jsonLiveKeywordDataV1.js?v=1"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        res = requests.get(url, headers=headers)
        res.encoding = 'euc-kr'
        
        data = json.loads(res.text)
        results = []
        for item in data:
            results.append({
                "rank": item[0],
                "keyword": item[1],
                "change": item[2] # ìˆœìœ„ ë³€ë™ (s:ë™ì¼, +:ìƒìŠ¹, -:í•˜ë½, n:NEW)
            })
            
        return results
    except Exception as e:
        print(f"Realtime keywords error: {e}")
        return []

def get_related_keywords(keyword):
    """
    ë„¤ì´ë²„ í†µí•©ê²€ìƒ‰(PC/ëª¨ë°”ì¼) ë° ìë™ì™„ì„± APIë¥¼ ì¢…í•©í•˜ì—¬ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    keywords = set()
    
    # 1. ë„¤ì´ë²„ ìë™ì™„ì„± API (ê¸°ë³¸ ì„±ëŠ¥ ë³´ì¥)
    try:
        ac_url = "https://ac.search.naver.com/nx/ac"
        ac_params = {
            "q": keyword, "con": "0", "frm": "nv", "ans": "2",
            "r_format": "json", "r_enc": "UTF-8", "rev": "4",
            "q_enc": "UTF-8", "st": "100"
        }
        res_ac = requests.get(ac_url, params=ac_params, timeout=5)
        if res_ac.status_code == 200:
            ac_data = res_ac.json()
            for group in ac_data.get('items', []):
                for item in group:
                    keywords.add(item[0])
    except Exception as e:
        print(f"Autocomplete Error: {e}")

    # 2. í†µí•©ê²€ìƒ‰ í˜ì´ì§€ ë¶„ì„ (ì—°ê´€ê²€ìƒ‰ì–´ + ìŠ¤ë§ˆíŠ¸ë¸”ë¡ ì œëª©)
    search_urls = [
        # PC
        (f"https://search.naver.com/search.naver?query={keyword}", 
         {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}),
        # Mobile (ìŠ¤ë§ˆíŠ¸ë¸”ë¡ ë…¸ì¶œì´ ë” ë§ìŒ)
        (f"https://m.search.naver.com/search.naver?query={keyword}",
         {'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36'})
    ]

    stop_words = ["ë‰´ìŠ¤", "ì´ë¯¸ì§€", "ì¸ê¸°ê¸€", "ë”ë³´ê¸°", "ì „ì²´", "ì¹´í˜", "ë¸”ë¡œê·¸", "ì§€ì‹iN", "ì¸í”Œë£¨ì–¸ì„œ", "ë™ì˜ìƒ", "ì‡¼í•‘", "ì§€ë„", "ê¸°íƒ€"]

    for url, headers in search_urls:
        try:
            res = requests.get(url, headers=headers, timeout=5)
            if res.status_code != 200: continue
            
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # ì „í†µì  ì—°ê´€ê²€ìƒ‰ì–´ selector
            tags = soup.select('.lst_related_srch .tit, .related_srch .tit, .keyword_box .tit, .related_srch .name')
            for tag in tags:
                keywords.add(tag.get_text(strip=True))

            # ìŠ¤ë§ˆíŠ¸ë¸”ë¡ ì œëª© (subjectTitle) - HTML ë‚´ JSON ë°ì´í„° íŒŒì‹±
            json_matches = re.findall(r'"subjectTitle"\s*:\s*"([^"]+)"', res.text)
            for m in json_matches:
                # ë¶ˆí•„ìš”í•œ ë…¸ì´ì¦ˆ ì œê±°
                if m and m not in stop_words and len(m) < 40:
                    keywords.add(m)

            # ëª¨ë°”ì¼ ìŠ¤ë§ˆíŠ¸ë¸”ë¡ íƒ€ì´í‹€ selector
            sb_titles = soup.select('.api_title_area .tit_main, .fds-comps-header-title')
            for sb in sb_titles:
                txt = sb.get_text(strip=True)
                if txt and txt not in stop_words and len(txt) < 40:
                    keywords.add(txt)

        except Exception as e:
            print(f"Search Page Error ({url[:30]}...): {e}")

    # ìµœì¢… ê²°ê³¼ ì •ì œ
    final_list = []
    for k in keywords:
        # ê²€ìƒ‰ì–´ ìì‹  ì œì™¸ ë° ë¶ˆìš©ì–´ í•„í„°ë§
        if k == keyword: continue
        if any(sw == k for sw in stop_words): continue
        if len(k) < 2: continue # ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œ ì œì™¸
        final_list.append(k)
        
    # ì¤‘ë³µ ì œê±° ë° ê°€ë‚˜ë‹¤ ìˆœ ì •ë ¬
    final_list = sorted(list(set(final_list)))
    
    return final_list

def get_keyword_info(keyword):
    """
    ë‹¨ì¼ í‚¤ì›Œë“œì— ëŒ€í•œ ê¸°ë³¸ ì •ë³´(ë¬¸ì„œìˆ˜ ë“±)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    """
    max_retries = 2
    for i in range(max_retries + 1):
        result = search_blog(keyword, display=1)
        if result and 'error' not in result:
            return {'total': result.get('total', 0)}
        
        err_msg = result.get('error', 'Unknown') if result else 'Empty'
        print(f"DEBUG: get_keyword_info retry {i+1} for '{keyword}': {err_msg}")
        if i < max_retries:
            time.sleep(0.5) # Wait before retry
            
    return {'total': 0, 'error': f"Failed after {max_retries} retries"}


def generate_signature(timestamp, method, uri, secret_key):
    message = f"{timestamp}.{method}.{uri}"
    signature = hmac.new(bytes(secret_key, "utf-8"), bytes(message, "utf-8"), hashlib.sha256).digest()
    return base64.b64encode(signature).decode("utf-8")

def get_search_volume(keyword):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  API (RelKwdStat)ë¥¼ í†µí•´ ê²€ìƒ‰ì–´ì˜ PC/ëª¨ë°”ì¼ ì¡°íšŒìˆ˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        # Load keys
        license_key = os.getenv("NAVER_AD_ACCESS_LICENSE")
        secret_key = os.getenv("NAVER_AD_SECRET_KEY")
        customer_id = os.getenv("NAVER_AD_CUSTOMER_ID")
        
        if not license_key or not secret_key or not customer_id:
            return None
            
        base_url = "https://api.naver.com"
        uri = "/keywordstool"
        method = "GET"
        timestamp = str(int(time.time() * 1000))
        
        signature = generate_signature(timestamp, method, uri, secret_key)
        
        headers = {
            "Content-Type": "application/json; charset=UTF-8",
            "X-Timestamp": timestamp,
            "X-API-KEY": license_key,
            "X-Customer": customer_id, 
            "X-Signature": signature
        }
        
        # hintKeywords allows comma separated list, but we search one by one or batch if needed.
        # Here we just search for the specific keyword.
        # Spaces in hintKeywords cause 400 error, so remove them
        params = {
            "hintKeywords": keyword.replace(" ", ""),
            "showDetail": "1"
        }
        
        res = requests.get(base_url + uri, params=params, headers=headers)
        if res.status_code == 200:
            data = res.json()
            keyword_list = data.get('keywordList', [])
            
            # Find exact match
            for item in keyword_list:
                if item['relKeyword'].replace(" ", "") == keyword.replace(" ", ""):
                    pc_vol = item.get('monthlyPcQcCnt', 0)
                    mo_vol = item.get('monthlyMobileQcCnt', 0)
                    
                    # API returns '< 10' as string for low volume
                    if isinstance(pc_vol, str): pc_vol = 0
                    if isinstance(mo_vol, str): mo_vol = 0
                    
                    comp_idx = item.get('compIdx', 'N/A')
                    
                    return {
                        'pc': pc_vol,
                        'mobile': mo_vol,
                        'total': pc_vol + mo_vol,
                        'comp_idx': comp_idx
                    }
                    
            # If no exact match found in list (should be there if hinted)
            if keyword_list:
                # Fallback to first item? No, risky. 
                pass
                
            return {'pc': 0, 'mobile': 0, 'total': 0}
            
        else:
            print(f"Ad API Error: {res.status_code} {res.text}")
            return None
            
    except Exception as e:
        print(f"Search volume error: {e}")
        return None

        return None

def get_search_volumes_for_keywords(keyword_list):
    """
    ì£¼ì–´ì§„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ê²€ìƒ‰ëŸ‰ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    5ê°œì”© ëŠì–´ì„œ Ad APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì •í™•í•œ ë°ì´í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    Returns: dict { 'keyword_nospace': {'original_keyword': str, 'pc': int, 'mobile': int, 'total': int} }
    """
    try:
        # Load keys
        license_key = os.getenv("NAVER_AD_ACCESS_LICENSE")
        secret_key = os.getenv("NAVER_AD_SECRET_KEY")
        customer_id = os.getenv("NAVER_AD_CUSTOMER_ID")
        
        if not license_key or not secret_key or not customer_id:
            return {}
            
        base_url = "https://api.naver.com"
        uri = "/keywordstool"
        method = "GET"
        
        headers = {
            "Content-Type": "application/json; charset=UTF-8",
            "X-API-KEY": license_key,
            "X-Customer": customer_id
        }
        
        final_result = {}
        
        # 5ê°œì”© ë°°ì¹­
        chunk_size = 5
        for i in range(0, len(keyword_list), chunk_size):
            chunk = keyword_list[i:i + chunk_size]
            
            # Clean keywords: remove special characters that might cause 400 error
            clean_chunk = []
            for k in chunk:
                # Remove non-alphanumeric except for space
                cleaned = re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', k)
                if cleaned.strip():
                    clean_chunk.append(cleaned.replace(" ", ""))
            
            if not clean_chunk:
                continue
                
            hint_str = ",".join(clean_chunk)
            
            timestamp = str(int(time.time() * 1000))
            signature = generate_signature(timestamp, method, uri, secret_key)
            headers.update({
                "X-Timestamp": timestamp,
                "X-Signature": signature
            })
            
            params = {
                "hintKeywords": hint_str,
                "showDetail": "1"
            }
            
            # API í˜¸ì¶œ
            res = requests.get(base_url + uri, params=params, headers=headers)
            
            if res.status_code == 200:
                data = res.json()
                for item in data.get('keywordList', []):
                    kwd = item['relKeyword']
                    # API ê²°ê³¼ê°€ ì…ë ¥í•œ í‚¤ì›Œë“œ(ê³µë°±ì œê±°)ì™€ ì¼ì¹˜í•˜ëŠ” ê²ƒë§Œ ì €ì¥ (ë˜ëŠ” ì „ì²´ ì €ì¥í•˜ì—¬ hit rate ë†’ì„)
                    # ì—¬ê¸°ì„œëŠ” ìš°ë¦¬ê°€ ìš”ì²­í•œ chunkì— ìˆëŠ” ê²ƒë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ì•„ì„œ ë§¤í•‘í•´ì•¼ í•¨.
                    # í•˜ì§€ë§Œ APIëŠ” ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ë” ë§ì´ ì¤Œ.
                    # íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¼ë‹¨ ë‹¤ ì €ì¥í•˜ë˜, keyëŠ” ê³µë°±ì œê±°ë¡œ í†µì¼
                    
                    pc_vol = item.get('monthlyPcQcCnt', 0)
                    mo_vol = item.get('monthlyMobileQcCnt', 0)
                    if isinstance(pc_vol, str): pc_vol = 0
                    if isinstance(mo_vol, str): mo_vol = 0
                    
                    comp_idx = item.get('compIdx', 'N/A')
                    
                    final_result[kwd.replace(" ", "")] = {
                        'original_keyword': kwd,
                        'pc': pc_vol,
                        'mobile': mo_vol,
                        'total': pc_vol + mo_vol,
                        'comp_idx': comp_idx
                    }
                # Rate limit safety (optional, but 5 keywords per call is rare)
                time.sleep(0.1)
            else:
                print(f"Batch Ad API Error: {res.status_code} {res.text}")
                
        return final_result
            
    except Exception as e:
        print(f"Bulk search volume error: {e}")
        return {}

def get_related_keywords_from_ad_api(seed_keyword):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ê´‘ê³  APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê´€ í‚¤ì›Œë“œ ëŒ€ëŸ‰(ìµœëŒ€ 1000ê°œ)ê³¼ ê·¸ ê²€ìƒ‰ëŸ‰ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        license_key = os.getenv("NAVER_AD_ACCESS_LICENSE")
        secret_key = os.getenv("NAVER_AD_SECRET_KEY")
        customer_id = os.getenv("NAVER_AD_CUSTOMER_ID")
        
        if not license_key or not secret_key or not customer_id:
            return []
            
        base_url = "https://api.naver.com"
        uri = "/keywordstool"
        method = "GET"
        timestamp = str(int(time.time() * 1000))
        signature = generate_signature(timestamp, method, uri, secret_key)
        
        headers = {
            "Content-Type": "application/json; charset=UTF-8",
            "X-Timestamp": timestamp,
            "X-API-KEY": license_key,
            "X-Customer": customer_id, 
            "X-Signature": signature
        }
        
        params = {
            "hintKeywords": re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', seed_keyword).replace(" ", ""),
            "showDetail": "1"
        }
        
        res = requests.get(base_url + uri, params=params, headers=headers)
        if res.status_code == 200:
            data = res.json()
            keyword_list = data.get('keywordList', [])
            
            results = []
            for item in keyword_list:
                pc_vol = item.get('monthlyPcQcCnt', 0)
                mo_vol = item.get('monthlyMobileQcCnt', 0)
                if isinstance(pc_vol, str): pc_vol = 0
                if isinstance(mo_vol, str): mo_vol = 0
                
                comp_idx = item.get('compIdx', 0)
                
                results.append({
                    'keyword': item['relKeyword'],
                    'pc': pc_vol,
                    'mobile': mo_vol,
                    'total': pc_vol + mo_vol,
                    'comp_idx': comp_idx
                })
            return results
        else:
            print(f"Ad API Error: {res.status_code}")
            return []
    except Exception as e:
        print(f"Discovery error: {e}")
        return []

def get_blog_rank(keyword):
    """
    í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ 'VIEW' ì˜ì—­ì˜ ìƒìœ„ ë…¸ì¶œ ì»¨í…ì¸ ê°€ ë¸”ë¡œê·¸ì¸ì§€ ì¹´í˜ì¸ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
    Returns: list of strings ("B" for Blog, "C" for Cafe, etc.)
    """
    try:
        # PC Search URL
        url = f"https://search.naver.com/search.naver?query={keyword}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')
        
        ranks = []
        
        # 'view_wrap' usually contains the list of VIEW results
        # Look for items in the VIEW section. New structure often has 'view_wrap'.
        view_section = soup.select_one(".view_wrap") or soup.select_one(".api_subject_bx")
        
        if view_section:
            items = view_section.select("li.bx")
            for item in items:
                text_content = item.get_text()
                # Simple heuristic check based on typical labels
                if "ë¸”ë¡œê·¸" in text_content:
                    ranks.append("B") 
                elif "ì¹´í˜" in text_content:
                    ranks.append("C") 
                else:
                    # Sometimes simpler structure
                    ranks.append("?")
                
                if len(ranks) >= 10:
                    break
        else:
            # Fallback for 'Smart Block' or different layouts
            # Just grab generic 'bx' items that look like content
            items = soup.select("li.bx")
            for item in items:
                txt = item.get_text()
                if "ë¸”ë¡œê·¸" in txt: ranks.append("B")
                elif "ì¹´í˜" in txt: ranks.append("C")
                if len(ranks) >= 10: break

        return ranks
    except Exception as e:
        print(f"Blog rank error: {e}")
        return []


def get_api_keys():
    client_id = os.getenv("NAVER_CLIENT_ID", "").strip()
    client_secret = os.getenv("NAVER_CLIENT_SECRET", "").strip()
    return client_id, client_secret

def search_blog(keyword, display=10, sort='sim'):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    :param keyword: ê²€ìƒ‰ì–´
    :param display: í‘œì‹œí•  ê²°ê³¼ ìˆ˜ (1~100)
    :param sort: ì •ë ¬ ìˆœì„œ (sim: ì •í™•ë„ìˆœ, date: ë‚ ì§œìˆœ)
    :return: ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (total, items ë“±) ë˜ëŠ” None (ì—ëŸ¬ ì‹œ)
    """
    client_id, client_secret = get_api_keys()
    
    if not client_id or not client_secret:
        return {"error": "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."}

    encText = requests.utils.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/blog?query={encText}&display={display}&sort={sort}"

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    try:
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Error Code: {response.status_code}", "details": response.text}
    except Exception as e:
        return {"error": str(e)}

def search_news(keyword, display=10, sort='sim'):
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return _search_general('news', keyword, display, sort)

def search_shop(keyword, display=10, sort='sim'):
    """
    ë„¤ì´ë²„ ì‡¼í•‘ ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return _search_general('shop', keyword, display, sort)

def search_kin(keyword, display=10, sort='sim'):
    """
    ë„¤ì´ë²„ ì§€ì‹iN ê²€ìƒ‰ APIë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    return _search_general('kin', keyword, display, sort)

def _search_general(service_type, keyword, display=10, sort='sim'):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ API ê³µí†µ í˜¸ì¶œ í•¨ìˆ˜
    """
    client_id, client_secret = get_api_keys()
    
    if not client_id or not client_secret:
        return {"error": "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."}

    encText = requests.utils.quote(keyword)
    url = f"https://openapi.naver.com/v1/search/{service_type}?query={encText}&display={display}&sort={sort}"

    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            return response.json()
        else:
            return {"error": f"Error Code: {response.status_code}", "details": response.text}
    except Exception as e:
        return {"error": str(e)}

def get_naver_section_order(keyword):
    """
    PCì™€ ëª¨ë°”ì¼ì˜ ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ ì„¹ì…˜ ìˆœì„œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    results = {'pc': [], 'mobile': []}
    
    # ì•Œë ¤ì§„ ì„¹ì…˜ ì´ë¦„ ëª©ë¡
    known_sections = ["ë‰´ìŠ¤", "ë¸”ë¡œê·¸", "ì‡¼í•‘", "ì§€ì‹iN", "ì´ë¯¸ì§€", "ì–´í•™ì‚¬ì „", "ì¸í”Œë£¨ì–¸ì„œ", "ì§€ë„", "ë™ì˜ìƒ", "ì›¹ë¬¸ì„œ"]
    noise = ["ë¬¸ì„œ ì €ì¥í•˜ê¸°", "Keepì— ì €ì¥", "Keep ë°”ë¡œê°€ê¸°", "AD", "ë„ì›€ë§", "VIEW", "ë”ë³´ê¸°"]
    
    # ì„¹ì…˜ ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜
    def extract_sections(soup, device='pc'):
        sections = []
        seen_titles = set()
        
        # ë°©ë²• 1: ì„¹ì…˜ ì»¨í…Œì´ë„ˆì—ì„œ ì§ì ‘ ì¶”ì¶œ (section.sc_new, div.api_subject_bx ë“±)
        section_containers = soup.select('section.sc_new, div.api_subject_bx, section[class*="sc_new"]')
        
        for container in section_containers:
            # ê° ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ í—¤ë” ì°¾ê¸°
            headers = container.select('.api_title, h2, .tit_main, .title_link, h3.title, .area_title')
            
            for h in headers:
                text = h.get_text(strip=True)
                if not text or text in seen_titles or text == keyword:
                    continue
                
                # ë…¸ì´ì¦ˆ í•„í„°ë§
                if text in noise or len(text) > 30:
                    continue
                
                # ì•Œë ¤ì§„ ì„¹ì…˜ì´ê±°ë‚˜ ì§§ì€ í…ìŠ¤íŠ¸ë©´ ì¶”ê°€
                if text in known_sections or (len(text) < 20 and text not in noise):
                    sections.append(text)
                    seen_titles.add(text)
        
        # ë°©ë²• 2: ëª¨ë“  í—¤ë” í›„ë³´ë¥¼ ë‹¤ì‹œ ê²€ì‚¬ (ë” í¬ê´„ì )
        all_headers = soup.select('.api_title, h2, .tit_main, .title_link, h3.title, .area_title')
        
        for h in all_headers:
            text = h.get_text(strip=True)
            if not text or text in seen_titles:
                continue
            
            if text in noise or len(text) > 30:
                continue
            
            # ë¶€ëª¨ íƒœê·¸ í™•ì¸í•˜ì—¬ ìœ íš¨í•œ ì„¹ì…˜ì¸ì§€ ê²€ì¦
            curr = h
            valid = False
            for _ in range(5):
                curr = curr.parent
                if not curr:
                    break
                classes = curr.get('class', [])
                if (curr.name == 'section' and 'sc_new' in ' '.join(classes)) or \
                   (curr.get('class') and 'api_subject_bx' in classes):
                    valid = True
                    break
            
            if valid:
                if text in known_sections or (len(text) < 20 and text not in noise):
                    if text not in seen_titles:
                        sections.append(text)
                        seen_titles.add(text)
        
        # ë°©ë²• 3: ì•Œë ¤ì§„ ì„¹ì…˜ ì´ë¦„ì´ HTMLì— ìˆìœ¼ë©´ ì§ì ‘ ê²€ìƒ‰ (ë‰´ìŠ¤ ë“± ë¹ ì§„ ê²½ìš° ëŒ€ë¹„)
        for known_sec in known_sections:
            if known_sec not in seen_titles:
                # HTMLì—ì„œ í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ìš”ì†Œ ì°¾ê¸°
                elements = soup.find_all(string=lambda s: s and known_sec in s.strip())
                for el in elements[:10]:  # ìµœëŒ€ 10ê°œë§Œ í™•ì¸
                    parent = el.parent
                    # ì„¹ì…˜ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì¸ì§€ í™•ì¸
                    for _ in range(5):
                        if not parent:
                            break
                        classes = parent.get('class', [])
                        if (parent.name == 'section' and 'sc_new' in ' '.join(classes)) or \
                           'api_subject_bx' in classes:
                            sections.append(known_sec)
                            seen_titles.add(known_sec)
                            break
                        parent = parent.parent
                    if known_sec in seen_titles:
                        break
        
        return sections
    
    # 1. PC ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
    try:
        url_pc = f"https://search.naver.com/search.naver?query={keyword}"
        headers_pc = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        res_pc = requests.get(url_pc, headers=headers_pc)
        soup_pc = BeautifulSoup(res_pc.text, 'html.parser')
        
        sections_pc = extract_sections(soup_pc, 'pc')
        results['pc'] = sections_pc
        
    except Exception as e:
        print(f"PC parsing error: {e}")
        results['pc'] = ["ì˜¤ë¥˜ ë°œìƒ"]

    # 2. ëª¨ë°”ì¼ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
    try:
        url_mo = f"https://m.search.naver.com/search.naver?query={keyword}"
        headers_mo = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G981B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36'
        }
        res_mo = requests.get(url_mo, headers=headers_mo)
        soup_mo = BeautifulSoup(res_mo.text, 'html.parser')
        
        sections_mo = extract_sections(soup_mo, 'mobile')
        results['mobile'] = sections_mo

    except Exception as e:
        print(f"Mobile parsing error: {e}")
        results['mobile'] = ["ì˜¤ë¥˜ ë°œìƒ"]
        
    return results

def get_google_trending_keywords(country_code='KR', limit=20):
    """
    êµ¬ê¸€ íŠ¸ë Œë“œì˜ ì‹¤ì‹œê°„ ê¸‰ìƒìŠ¹ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    pytrends ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„í•©ë‹ˆë‹¤.
    
    Args:
        country_code: êµ­ê°€ ì½”ë“œ (KR: í•œêµ­, US: ë¯¸êµ­ ë“±)
        limit: ê°€ì ¸ì˜¬ í‚¤ì›Œë“œ ê°œìˆ˜
    
    Returns:
        List of dict: [{"rank": 1, "keyword": "í‚¤ì›Œë“œ", "traffic": "N/A"}, ...]
    """
    results = []
    
    # ë°©ë²• 1: pytrends ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
    try:
        from pytrends.request import TrendReq
        
        # pytrends ì´ˆê¸°í™”
        pytrends = TrendReq(hl='ko', tz=540, timeout=(10, 25))
        
        # trending_searches í•¨ìˆ˜ ì‚¬ìš© (pandas DataFrame ë°˜í™˜)
        try:
            trending_df = pytrends.trending_searches(pn=country_code)
            
            if trending_df is not None and len(trending_df) > 0:
                # DataFrameì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                if hasattr(trending_df, 'iloc'):
                    rank = 1
                    for idx in range(min(limit, len(trending_df))):
                        keyword = str(trending_df.iloc[idx, 0]).strip()
                        if keyword and keyword != 'nan':
                            results.append({
                                "rank": rank,
                                "keyword": keyword,
                                "traffic": "N/A"
                            })
                            rank += 1
                else:
                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš°
                    keywords_list = list(trending_df[0]) if isinstance(trending_df, list) else trending_df.tolist()
                    rank = 1
                    for keyword in keywords_list[:limit]:
                        keyword_str = str(keyword).strip()
                        if keyword_str and keyword_str != 'nan':
                            results.append({
                                "rank": rank,
                                "keyword": keyword_str,
                                "traffic": "N/A"
                            })
                            rank += 1
                        
        except Exception as e:
            print(f"pytrends trending_searches error: {e}")
            import traceback
            traceback.print_exc()
            
    except ImportError:
        print("pytrends ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install pytrends'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"pytrends initialization error: {e}")
    
    # ë°©ë²• 2: pytrendsê°€ ì‹¤íŒ¨í•œ ê²½ìš° Google Trends JSON API ì‹œë„
    if not results:
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "*/*",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://trends.google.com/",
            }
            
            # Google Trendsì˜ ë‚´ë¶€ JSON API ì—”ë“œí¬ì¸íŠ¸
            url = f"https://trends.google.com/trends/api/dailytrends?hl=ko&geo={country_code}&ns=15"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200 and response.text:
                # Google Trends APIëŠ” ")]}',"ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ê°€ ìˆìŒ
                content = response.text
                if content.startswith(")]}',"):
                    content = content[5:]  # ì œê±°
                
                try:
                    data = json.loads(content)
                    
                    # JSON êµ¬ì¡° íŒŒì‹±
                    if 'default' in data and 'trendingSearchesDays' in data['default']:
                        for day_data in data['default']['trendingSearchesDays']:
                            if 'trendingSearches' in day_data:
                                rank = 1
                                for trend in day_data['trendingSearches']:
                                    if rank > limit:
                                        break
                                    
                                    if 'title' in trend and 'query' in trend['title']:
                                        keyword = trend['title']['query']
                                        
                                        # íŠ¸ë˜í”½ ì •ë³´
                                        traffic = "N/A"
                                        if 'formattedTraffic' in trend:
                                            traffic = trend['formattedTraffic']
                                        
                                        if keyword not in [r['keyword'] for r in results]:
                                            results.append({
                                                "rank": rank,
                                                "keyword": keyword,
                                                "traffic": traffic
                                            })
                                            rank += 1
                                
                                if results:
                                    break  # ì²« ë²ˆì§¸ ë‚  ë°ì´í„°ë§Œ ì‚¬ìš©
                                    
                except json.JSONDecodeError as e:
                    print(f"JSON Parse Error: {e}")
                    
        except Exception as e:
            print(f"Google Trends JSON API Error: {e}")
    
    # ë°©ë²• 3: RSS/Atom í”¼ë“œ ë°±ì—… (ë§ˆì§€ë§‰ ë°©ë²•)
    if not results:
        try:
            # êµ­ê°€ ì½”ë“œ ë§¤í•‘ (RSS í”¼ë“œìš©)
            country_map = {
                'KR': 'p23',
                'US': 'p1',
                'JP': 'p27',
                'CN': 'p36',
                'GB': 'p9'
            }
            pn = country_map.get(country_code, 'p23')
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/atom+xml,application/xml,text/xml,*/*",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://trends.google.com/",
            }
            
            # ì—¬ëŸ¬ URL íŒ¨í„´ ì‹œë„
            urls_to_try = [
                f"https://trends.google.com/trends/hottrends/atom/feed?pn={pn}",
                f"https://trends.google.co.kr/trends/hottrends/atom/feed?pn={pn}",
                f"https://trends.google.com/trends/trendingsearches/daily/rss?geo={country_code}",
            ]
            
            for url in urls_to_try:
                if len(results) >= limit:
                    break
                    
                try:
                    response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
                    
                    if response.status_code == 200 and response.content:
                        # XML/Atom íŒŒì‹± ì‹œë„
                        try:
                            # ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì ì œê±°
                            content = response.content.decode('utf-8', errors='ignore')
                            
                            # XML íŒŒì‹±
                            root = ET.fromstring(content)
                            
                            # Atom í”¼ë“œ í˜•ì‹ íŒŒì‹±
                            rank = 1
                            items = root.findall('.//item') or root.findall('.//entry')
                            
                            for item in items:
                                if rank > limit:
                                    break
                                
                                keyword = None
                                
                                # title íƒœê·¸ ì°¾ê¸° (ë‹¤ì–‘í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì§€ì›)
                                title_elem = item.find('.//title')
                                if title_elem is None:
                                    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
                                    for ns in ['', '{http://www.w3.org/2005/Atom}']:
                                        title_elem = item.find(f'{ns}title')
                                        if title_elem is not None:
                                            break
                                
                                if title_elem is not None and title_elem.text:
                                    keyword = title_elem.text.strip()
                                    
                                    # HTML íƒœê·¸ ì œê±°
                                    if '<' in keyword:
                                        keyword = re.sub(r'<[^>]+>', '', keyword).strip()
                                
                                if keyword and keyword not in [r['keyword'] for r in results]:
                                    # íŠ¸ë˜í”½ ì •ë³´ ì‹œë„
                                    traffic = "N/A"
                                    for elem in item.iter():
                                        tag_text = elem.tag if isinstance(elem.tag, str) else str(elem.tag)
                                        if 'traffic' in tag_text.lower() or 'approx' in tag_text.lower():
                                            if elem.text:
                                                traffic = elem.text.strip()
                                            break
                                    
                                    results.append({
                                        "rank": rank,
                                        "keyword": keyword,
                                        "traffic": traffic
                                    })
                                    rank += 1
                            
                            if results:
                                break  # ì„±ê³µí–ˆìœ¼ë©´ ë‹¤ìŒ URL ì‹œë„ ì•ˆ í•¨
                                
                        except ET.ParseError as e:
                            print(f"XML Parse Error for {url}: {e}")
                            continue
                        except Exception as e:
                            print(f"Parse Error for {url}: {e}")
                            continue
                            
                except requests.exceptions.RequestException as e:
                    print(f"Request Error for {url}: {e}")
                    continue
                except Exception as e:
                    print(f"Unexpected Error for {url}: {e}")
                    continue
                    
        except Exception as e:
            print(f"Google Trends RSS Error: {e}")
    
    return results

def get_datalab_shopping_trends(cid):
    """
    ë„¤ì´ë²„ ë°ì´í„°ë© ì‡¼í•‘ ì¸ì‚¬ì´íŠ¸ì—ì„œ ë¶„ì•¼ë³„ ì¸ê¸° ê²€ìƒ‰ì–´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    Ref: https://datalab.naver.com/shoppingInsight/sCategory.naver
    """
    url = "https://datalab.naver.com/shoppingInsight/getCategoryKeywordRank.naver"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
        "Referer": "https://datalab.naver.com/shoppingInsight/sCategory.naver",
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
        "Origin": "https://datalab.naver.com"
    }

    # ìµœê·¼ 2ì¼ ì „ ë°ì´í„°ë¥¼ ì¡°íšŒ (ë°ì´í„°ë© ì—…ë°ì´íŠ¸ ì£¼ê¸°ì— ë§ì¶¤)
    today = datetime.now()
    end_date = (today - timedelta(days=2)).strftime("%Y-%m-%d")
    start_date = (today - timedelta(days=3)).strftime("%Y-%m-%d")
    
    data = {
        "cid": str(cid),
        "timeUnit": "date",
        "startDate": start_date,
        "endDate": end_date,
        "age": "",
        "gender": "",
        "device": "",
        "page": 1,
        "count": 20
    }
    
    try:
        res = requests.post(url, headers=headers, data=data)
        if res.status_code == 200:
            result = res.json()
            # ranks: [{rank, keyword, linkId}, ...]
            return result.get('ranks', [])
        else:
            print(f"DataLab API Failed: {res.status_code}")
            return []
    except Exception as e:
        print(f"DataLab API Error: {e}")
        return []

def get_related_ad_keywords(keyword):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ ê´‘ê³  APIë¥¼ í†µí•´ ê´€ë ¨ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    try:
        license_key = os.getenv("NAVER_AD_ACCESS_LICENSE")
        secret_key = os.getenv("NAVER_AD_SECRET_KEY")
        customer_id = os.getenv("NAVER_AD_CUSTOMER_ID")
        
        if not license_key or not secret_key or not customer_id:
            print("Ad API keys missing")
            return []
            
        base_url = "https://api.naver.com"
        uri = "/keywordstool"
        method = "GET"
        timestamp = str(int(time.time() * 1000))
        signature = generate_signature(timestamp, method, uri, secret_key)
        
        headers = {
            "Content-Type": "application/json; charset=UTF-8",
            "X-Timestamp": timestamp,
            "X-API-KEY": license_key,
            "X-Customer": customer_id, 
            "X-Signature": signature
        }
        
        params = {
            "hintKeywords": keyword.replace(" ", ""),
            "showDetail": "1"
        }
        
        res = requests.get(base_url + uri, params=params, headers=headers)
        if res.status_code == 200:
            data = res.json()
            return data.get('keywordList', [])
        else:
            print(f"Ad API Error: {res.status_code} {res.text}")
            return []
    except Exception as e:
        print(f"Error fetching AD keywords: {e}")
        return []

def find_golden_keywords(seed_keyword, min_search_vol=500, top_n=30):
    """
    í™©ê¸ˆ í‚¤ì›Œë“œë¥¼ ë°œêµ´í•©ë‹ˆë‹¤.
    1. ê²€ìƒ‰ê´‘ê³  APIë¡œ ê´€ë ¨ í‚¤ì›Œë“œ ë° ê²€ìƒ‰ëŸ‰ ìˆ˜ì§‘
    2. í•„í„°ë§ ë° ìƒìœ„ í‚¤ì›Œë“œ ì„ ì •
    3. ë¸”ë¡œê·¸ ê²€ìƒ‰ APIë¡œ ë¬¸ì„œìˆ˜ ìˆ˜ì§‘
    4. í™©ê¸ˆì§€ìˆ˜ ê³„ì‚° ë° ì •ë ¬
    """
    print(f"Starting Golden Keyword discovery for: {seed_keyword}")
    
    # 1. ë¦´ë ˆì´ì…˜ í‚¤ì›Œë“œ ìˆ˜ì§‘
    ad_keywords = get_related_ad_keywords(seed_keyword)
    if not ad_keywords:
        return []
    
    # 2. 1ì°¨ í•„í„°ë§ ë° ì •ë ¬ (ê²€ìƒ‰ëŸ‰ ê¸°ì¤€)
    candidates = []
    for item in ad_keywords:
        kw = item['relKeyword']
        pc_vol = item.get('monthlyPcQcCnt', 0)
        mo_vol = item.get('monthlyMobileQcCnt', 0)
        
        # APIê°€ '< 10'ì„ ë¬¸ìì—´ë¡œ ì£¼ê¸°ë„ í•¨
        if isinstance(pc_vol, str): pc_vol = 5
        if isinstance(mo_vol, str): mo_vol = 5
        
        total_vol = pc_vol + mo_vol
        
        if total_vol >= min_search_vol:
            candidates.append({
                'keyword': kw,
                'pc_vol': pc_vol,
                'mo_vol': mo_vol,
                'total_vol': total_vol
            })
            
    # ê²€ìƒ‰ëŸ‰ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œ ì„ ì •
    candidates = sorted(candidates, key=lambda x: x['total_vol'], reverse=True)[:top_n]
    
    # 3. ë¬¸ì„œ ìˆ˜ ìˆ˜ì§‘ ë° í™©ê¸ˆì§€ìˆ˜ ê³„ì‚°
    results = []
    for item in candidates:
        kw = item['keyword']
        # ë¸”ë¡œê·¸ ê²€ìƒ‰ APIë¡œ ë¬¸ì„œìˆ˜(total) ê°€ì ¸ì˜¤ê¸°
        blog_info = get_keyword_info(kw)
        doc_count = blog_info.get('total', 0)
        
        # í™©ê¸ˆë¥ (ê²½ìŸ ê°•ë„) = (ë¬¸ì„œ ìˆ˜ / ì›”ê°„ ê²€ìƒ‰ëŸ‰)
        # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ê²€ìƒ‰ëŸ‰ ëŒ€ë¹„ ë¬¸ì„œê°€ ì ìŒ)
        competition_rate = round((doc_count / item['total_vol']) if item['total_vol'] > 0 else 999, 2)
        
        results.append({
            'keyword': kw,
            'total_vol': item['total_vol'],
            'pc_vol': item['pc_vol'],
            'mo_vol': item['mo_vol'],
            'doc_count': doc_count,
            'competition_rate': competition_rate
        })
        # API ì†ë„ ì œí•œ ê³ ë ¤
        time.sleep(0.1)
        
    # ê²½ìŸë¥  ë‚®ì€ ìˆœ(í™©ê¸ˆ í‚¤ì›Œë“œ)ìœ¼ë¡œ ì •ë ¬
    results = sorted(results, key=lambda x: x['competition_rate'])
    
    return results


def analyze_top_blogs(keyword, count=5):
    """
    í‚¤ì›Œë“œì˜ ìƒìœ„ ë¸”ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ê²½ìŸ ë‚œì´ë„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        dict: {
            'keyword': str,
            'top_posts': list of post info,
            'avg_length': int (í‰ê·  ê¸€ììˆ˜ ì¶”ì •),
            'difficulty': str (ì‰¬ì›€/ë³´í†µ/ì–´ë ¤ì›€),
            'difficulty_score': int (0-100),
            'recommendation': str
        }
    """
    try:
        # 1. ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ APIë¡œ ìƒìœ„ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        blog_result = search_blog(keyword, display=count, sort='sim')
        
        if 'error' in blog_result:
            return {'error': blog_result['error'], 'keyword': keyword}
        
        items = blog_result.get('items', [])
        if not items:
            return {
                'keyword': keyword,
                'top_posts': [],
                'avg_length': 0,
                'difficulty': 'ì•Œìˆ˜ì—†ìŒ',
                'difficulty_score': 0,
                'recommendation': 'ë°ì´í„° ì—†ìŒ'
            }
        
        # 2. ê° í¬ìŠ¤íŠ¸ ë¶„ì„
        top_posts = []
        total_desc_len = 0
        fresh_count = 0  # ìµœê·¼ 30ì¼ ë‚´ í¬ìŠ¤íŠ¸ ìˆ˜
        
        for item in items:
            title = BeautifulSoup(item.get('title', ''), 'html.parser').get_text()
            description = BeautifulSoup(item.get('description', ''), 'html.parser').get_text()
            blogger_name = item.get('bloggername', '')
            post_date_str = item.get('postdate', '')
            link = item.get('link', '')
            
            # ë‚ ì§œ íŒŒì‹± (YYYYMMDD í˜•ì‹)
            days_ago = 999
            if post_date_str and len(post_date_str) == 8:
                try:
                    post_date = datetime.strptime(post_date_str, '%Y%m%d')
                    days_ago = (datetime.now() - post_date).days
                    if days_ago <= 30:
                        fresh_count += 1
                except:
                    pass
            
            # ì„¤ëª… ê¸¸ì´ (ì‹¤ì œ ê¸€ììˆ˜ì˜ ëŒ€ëµì  ì§€í‘œ)
            desc_len = len(description)
            total_desc_len += desc_len
            
            top_posts.append({
                'title': title,
                'blogger': blogger_name,
                'desc_length': desc_len,
                'days_ago': days_ago,
                'link': link
            })
        
        # 3. ë‚œì´ë„ ê³„ì‚°
        avg_desc_len = total_desc_len // len(items) if items else 0
        freshness_ratio = fresh_count / len(items) if items else 0
        
        # ë‚œì´ë„ ì ìˆ˜ (0-100)
        # ì„¤ëª… ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡, ìµœì‹  ê¸€ì´ ë§ì„ìˆ˜ë¡ ê²½ìŸì´ ì¹˜ì—´í•¨
        length_score = min(avg_desc_len / 3, 50)  # ìµœëŒ€ 50ì 
        freshness_score = freshness_ratio * 50   # ìµœëŒ€ 50ì 
        difficulty_score = int(length_score + freshness_score)
        
        # ë“±ê¸‰ íŒì •
        if difficulty_score < 30:
            difficulty = 'ğŸŸ¢ ì‰¬ì›€'
            recommendation = 'ì§€ê¸ˆ ë°”ë¡œ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”! ê²½ìŸì´ ë‚®ìŠµë‹ˆë‹¤.'
        elif difficulty_score < 60:
            difficulty = 'ğŸŸ¡ ë³´í†µ'
            recommendation = 'ì–‘ì§ˆì˜ ì½˜í…ì¸ ë¡œ ì¶©ë¶„íˆ ìƒìœ„ë…¸ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤.'
        else:
            difficulty = 'ğŸ”´ ì–´ë ¤ì›€'
            recommendation = 'ì¥ë¬¸ì˜ ê³ í€„ë¦¬í‹° ì½˜í…ì¸ ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì¸í”Œë£¨ì–¸ì„œ ê²½ìŸ ì£¼ì˜.'
        
        return {
            'keyword': keyword,
            'top_posts': top_posts,
            'avg_length': avg_desc_len * 10,  # ì„¤ëª… â†’ ì˜ˆìƒ ë³¸ë¬¸ ê¸¸ì´ë¡œ í™˜ì‚°
            'difficulty': difficulty,
            'difficulty_score': difficulty_score,
            'freshness_ratio': round(freshness_ratio * 100),
            'recommendation': recommendation
        }
        
    except Exception as e:
        print(f"analyze_top_blogs error: {e}")
        return {'error': str(e), 'keyword': keyword}
